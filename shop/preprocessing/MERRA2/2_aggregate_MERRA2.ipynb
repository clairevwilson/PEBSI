{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import zarr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Aggregating MERRA-2 data\n",
    "### Written by Claire Wilson (cvwilson@andrew.cmu.edu)\n",
    "\n",
    "Once you have all the daily files downloaded from MERRA-2, you'll need to aggregate them. This notebook will walk you through concatenating files into a larger zarr. \n",
    "\n",
    "If you saved a large region of interest, you will then pull out individual grid cells for model simulations using `3_get_tile.py`. In this case, it's easy to store the larger zarr on an external hard drive and run this code to pull out a smaller subset of data to store on your local machine.\n",
    "\n",
    "If you only are interested in a single grid cell to begin with, you should name your region of interest to be \"{lat}_{lon}\" where lat and lon are the centerpoint of the MERRA-2 grid cell you accessed. Example:\n",
    "\n",
    "`roi = '62.5_-145.625'`\n",
    "\n",
    "The model expects .nc files, so if you've created .zarrs containing a single grid cell, simply rename these to .nc files and delete the .zarrs. The files are ready for the model to digest.\n",
    "\n",
    "### 1. Specify the base filepath and name for the region of interest.\n",
    "\n",
    "The zarr_store filepath points to where you want to store the zarrs. The region of interest is a descriptive name for the region you downloaded which will be included in the file name of the zarr store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file naming\n",
    "base_fp = '../MERRA-2/'\n",
    "fp_zarr_store = base_fp + 'zarr_store/'\n",
    "roi = 'reg01'\n",
    "\n",
    "# Copy over your bounding box from notebook #1\n",
    "# This is just used to make sure all the files have the exact same lat/lon\n",
    "lat_min = 50\n",
    "lat_max = 72\n",
    "lon_min = -180\n",
    "lon_max = -133.25\n",
    "\n",
    "# =============================================================================================================================\n",
    "# Do not edit these\n",
    "dataset_variables = {\n",
    "    'slv':['T2M','U2M','V2M','QV2M','PS'],\n",
    "    'adg':['BCDP002','BCWT002','OCDP002','OCWT002','DUDP003','DUWT003'],\n",
    "    'rad':['SWGDN','LWGAB','CLDTOT'], \n",
    "    'flx':['PRECTOTCORR']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aggregate files by variable.\n",
    "\n",
    "The following code will loop through all the .nc4 files within the dataset (adg, slv, rad, or flx) folder where you should've saved the daily files from MERRA-2. Make sure there are no different .nc4 files in this folder. \n",
    "\n",
    "All you should need to do with this code is specify the dataset filepath and let it chug away. Expect this to take anywhere from 1 to 10 minutes per dataset, depending on how much data you are concatenating. In my experience, concatenating 20 years of data for a single variable takes about 4 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shouldn't need to make any edits to this cell\n",
    "def process_files(dataset, data_fp, roi=roi, filetype='.nc4'):\n",
    "    \"\"\"\n",
    "    This function loops through a user-specified folder which\n",
    "    contains daily files downloaded from MERRA-2 in Notebook #1.\n",
    "    The daily files should all be the same lat/lon bounds. They\n",
    "    will be appended via time onto the existing zarr if it exists,\n",
    "    otherwise the zarr will be created for each var in the files.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    dataset : str\n",
    "        Name of dataset from set [slv, adg, rad, flx]\n",
    "    data_fp : str\n",
    "        Filepath to folder containing files to append\n",
    "    roi : str \n",
    "        Descriptive name for region of interest for output zarr\n",
    "    filetype : str\n",
    "        File extension of the files to append (should be .nc4 or .nc)\n",
    "    \"\"\"\n",
    "    # get list of .nc4 files in the folder\n",
    "    daily_files = sorted(glob.glob(data_fp + '*'+filetype))\n",
    "\n",
    "    # check which times already exist for which vars\n",
    "    times_var = {}\n",
    "    for var in dataset_variables[dataset]:\n",
    "        fn_zarr_var = os.path.join(fp_zarr_store, var, f'{var}_{roi}.zarr')\n",
    "        if os.path.exists(fn_zarr_var):\n",
    "            # open existing zarr and make a set of timestamps already in it\n",
    "            ds_existing = xr.open_zarr(fn_zarr_var)\n",
    "            existing_time = pd.to_datetime(ds_existing.time.values)\n",
    "            times_var[var] = set(existing_time)\n",
    "        else:\n",
    "            # zarr does not exist; times is an empty set\n",
    "            times_var[var] = set()\n",
    "\n",
    "    # keep track of how many files were actually appended\n",
    "    n_added = 0\n",
    "    for i, f in tqdm(enumerate(daily_files), total=len(daily_files), desc=f'Processing {dataset}'):\n",
    "        ds = xr.open_dataset(f)\n",
    "\n",
    "        # loop through variables in the dataset\n",
    "        for var in dataset_variables[dataset]:\n",
    "            da = ds[var].sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))\n",
    "\n",
    "            # get zarr path for this variable\n",
    "            fn_zarr_var = os.path.join(fp_zarr_store, var, f'{var}_{roi}.zarr')\n",
    "\n",
    "            # get first timestamp of the file without opening it\n",
    "            assert 'Nx.' in f, 'File name is in an unexpected format: manually specify how to pull timestamp from the filename'\n",
    "            time_file = pd.to_datetime(f.split('Nx.')[-1][:8]) + pd.Timedelta(minutes=30)\n",
    "\n",
    "            # check if the time is already in the database\n",
    "            if time_file in times_var[var]:\n",
    "                # skip days that are already in the file\n",
    "                continue\n",
    "            elif var == dataset_variables[dataset][0]:\n",
    "                n_added += 1\n",
    "\n",
    "            # add this timestamp to the check list\n",
    "            times_var[var].add(time_file)\n",
    "\n",
    "            # check if we need to make the dataset\n",
    "            if i == 0 and not os.path.exists(fn_zarr_var):\n",
    "                # dataset does not exist so create it with the first file\n",
    "                da.to_zarr(\n",
    "                    fn_zarr_var,\n",
    "                    mode='w',\n",
    "                    consolidated=True,\n",
    "                )\n",
    "            else:\n",
    "                # append daily dataset onto the zarr\n",
    "                da.to_dataset(name=var).to_zarr(\n",
    "                    fn_zarr_var,\n",
    "                    mode='a',\n",
    "                    append_dim='time',\n",
    "                )\n",
    "\n",
    "    print(f'Successfully concatenated {n_added} files to {dataset}_{roi}')\n",
    "\n",
    "    # loop through variables and sort by time\n",
    "    for var in dataset_variables[dataset]:\n",
    "        # define file names\n",
    "        fn_zarr_var = os.path.join(fp_zarr_store, var, f'{var}_{roi}.zarr')\n",
    "        fn_zarr_unsorted = os.path.join(fp_zarr_store, var, f'{var}_{roi}_unsorted.zarr')\n",
    "\n",
    "        # create a unique filename for the unsorted dataset \n",
    "        # (just in case you mess something up and need to run this more than once)\n",
    "        ii = 0\n",
    "        while os.path.exists(fn_zarr_unsorted.replace('unsorted', f'unsorted_{ii}')):\n",
    "            ii += 1 \n",
    "        fn_zarr_unsorted = fn_zarr_unsorted.replace('unsorted', f'unsorted_{ii}')\n",
    "\n",
    "        # rename the unsorted dataset you just built to the unique filename\n",
    "        os.rename(fn_zarr_var, fn_zarr_unsorted)\n",
    "\n",
    "        # open the unsorted dataset\n",
    "        ds = xr.open_zarr(fn_zarr_unsorted)\n",
    "\n",
    "        # sort the dataset and check there are no time duplicates\n",
    "        ds_sorted = ds.sortby('time')\n",
    "        ds = ds.sel(time=~ds.time.to_index().duplicated())\n",
    "\n",
    "        # rechunk dataset and clear the encoding for safe saving\n",
    "        ds_sorted = ds_sorted.chunk({'time':8760, 'lat':20, 'lon':16})\n",
    "        for v in ds_sorted.variables:\n",
    "            ds_sorted[v].encoding.clear()\n",
    "\n",
    "        # save the sorted dataset to the original filename\n",
    "        ds_sorted.to_zarr(fn_zarr_var, mode='w', consolidated=True)\n",
    "        \n",
    "        # reconsolidate the data\n",
    "        store = zarr.DirectoryStore(fn_zarr_var)\n",
    "        zarr.consolidate_metadata(store)\n",
    "\n",
    "    print(f'Resorted and reconsolidated all vars in {dataset}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['slv']:\n",
    "    # Specify the folder to find files\n",
    "    data_fp = base_fp + dataset + '/'\n",
    "\n",
    "    # Process all files in the folder\n",
    "    process_files(dataset, data_fp)\n",
    "    # If files are not in .nc4 format, change the file type using filetype='.XXXX' argument in process_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (Recommended) Visualize all the data to ensure files are saved properly and do not contain gaps.\n",
    "\n",
    "The following code will help you check that this notebook worked correctly by producing the following:\n",
    "\n",
    "1. Map of the region of interest containing missing data percentage for each grid cell. Purple = no missing data = good.\n",
    "2. Timeseries collapsed by lat/lon. Should not contain any gaps.\n",
    "3. Printout of the dataset itself for you to check the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all datasets and all variables \n",
    "for dataset in dataset_variables:\n",
    "    for var in dataset_variables[dataset]:\n",
    "        # Figure 1: missing data map\n",
    "        plt.figure()\n",
    "\n",
    "        # open the dataset and get the data array\n",
    "        ds = xr.open_zarr(f'../MERRA-2/zarr_store/{var}/{var}_{roi}.zarr')\n",
    "        da = ds[var]\n",
    "        n_time = da.sizes['time']\n",
    "\n",
    "        # fraction of missing timesteps per lat/lon cell\n",
    "        frac_missing = da.isnull().sum(dim='time') / n_time\n",
    "\n",
    "        # plot the missing fraction\n",
    "        frac_missing.plot(cmap='viridis', vmin=0, vmax=1)\n",
    "        plt.title('Data missing percentage over Alaska region')\n",
    "\n",
    "        # save the figure \n",
    "        if not os.path.exists(base_fp + 'Figs/'):\n",
    "            os.mkdir(base_fp + 'Figs/')\n",
    "        plt.savefig(base_fp + f'Figs/{var}_missing_area.png',dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # Figure 2: timeseries\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # sum or average over lat/lon based on the variable\n",
    "        if 'OC' in var or 'PREC' in var or 'BC' in var or 'DU' in var:\n",
    "            da = ds[var].sum(dim=['lat','lon'])\n",
    "        else:\n",
    "            da = ds[var].mean(dim=['lat','lon'])\n",
    "\n",
    "        # plot the timeseries\n",
    "        ax.plot(da.time.values, da.values)\n",
    "\n",
    "        # descriptive title\n",
    "        start = pd.to_datetime(da.time.values[0]).strftime('%d %b %Y')\n",
    "        end = pd.to_datetime(da.time.values[-1]).strftime('%d %b %Y')\n",
    "        ax.set_ylabel(var)\n",
    "        ax.set_title(f'{var} timeseries for the Alaska region\\n({start} to {end})')\n",
    "\n",
    "        # store figure\n",
    "        plt.savefig(base_fp + f'Figs/{var}_full_timeseries.png',dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # print dataset\n",
    "        print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, your zarrs are complete!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
