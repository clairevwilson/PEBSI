{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaec0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse, unquote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3e68d",
   "metadata": {},
   "source": [
    "## I. Downloading MERRA-2 data\n",
    "\n",
    "This notebook contains the code for the first step of preparing MERRA-2 data for the PEBSI model. It walks through creating the download files for the user-specified region and dates of interest.\n",
    "\n",
    "Hopefully this all works right out of the box and is super streamlined, but feel free to contact the author (cvwilson@cmu.edu) if you run into any issues you can't solve.\n",
    "\n",
    "First, specify data_fp which should be some folder where you want everything to be stored; recommended to name it MERRA-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dab0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fp= '../MERRA-2/'\n",
    "\n",
    "# =============================================================================================================================\n",
    "# Do not touch this code, it is needed later\n",
    "filename = 'MERRA2_VERSION.tavg1_2d_DATASET_Nx.DATE.nc4.nc4'\n",
    "def version(year,new_version=False):\n",
    "    if year < 1992:\n",
    "        version = '100'\n",
    "    elif year <= 2000:\n",
    "        version = '200'\n",
    "    elif year <= 2010:\n",
    "        version = '300'\n",
    "    else:\n",
    "        version = '400'\n",
    "    if new_version:\n",
    "        version = version[0:2]\n",
    "        version = version + '1'\n",
    "    return version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de7a9c",
   "metadata": {},
   "source": [
    "### 1. Get a global sample file. (Only need to do this once.)\n",
    "\n",
    "Download a reference file which contains the geopotential for every grid cell here:\n",
    "https://opendap.earthdata.nasa.gov/collections/C1276812819-GES_DISC/granules/M2C0NXASM.5.12.4%3AMERRA2_101.const_2d_asm_Nx.00000000.nc4.dap.nc4?dap4.ce=/PHIS;/time;/lat;/lon\n",
    "\n",
    "You will need to create or log in with your NASA EarthData login to get this file. While you're at it, set up your .netrc file which stores your username and password so you can download the rest of the MERRA-2 data. Check out documentation online for more information on this: https://nsidc.org/data/user-resources/help-center/creating-netrc-file-earthdata-login\n",
    "\n",
    "### 2. Specify the latitude and longitude of a bounding box.\n",
    "\n",
    "The following code will extract the integers which will be filled into each URL to fetch the correct file.\n",
    "\n",
    "Specify fn_gp and the lat/lon min/max in degrees. (Use negatives for west longitudes / south latitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d91e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the filepath where you saved the global sample file\n",
    "fn_gp = data_fp + 'MERRA2constants.nc4'\n",
    "ds_gp = xr.open_dataset(fn_gp)\n",
    "ds_gp = ds_gp.drop_dims('time')\n",
    "\n",
    "# specify the bounding box\n",
    "lat_min = 50      # ALASKA\n",
    "lat_max = 72\n",
    "lon_min = -180\n",
    "lon_max = -133.25\n",
    "\n",
    "# find the integer values bounding this lat/lon box\n",
    "lat_min_idx = np.where(ds_gp.lat.values >= lat_min)[0][0]\n",
    "lat_max_idx = np.where(ds_gp.lat.values <= lat_max)[0][-1]\n",
    "lon_min_idx = np.where(ds_gp.lon.values >= lon_min)[0][0]\n",
    "lon_max_idx = np.where(ds_gp.lon.values <= lon_max)[0][-1]\n",
    "print(f'latitude bounded by {lat_min_idx}:{lat_max_idx}')\n",
    "print(f'longitude bounded by {lon_min_idx}:{lon_max_idx}')\n",
    "\n",
    "# Sanity check: make sure the lon/lat lines up with what you specified\n",
    "print(ds_gp.isel(lat=np.arange(lat_min_idx, lat_max_idx + 1), \n",
    "                 lon=np.arange(lon_min_idx, lon_max_idx + 1)).coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4f3ed",
   "metadata": {},
   "source": [
    "### 3. Specify the time bounds and dataset to download.\n",
    "\n",
    "Specify start and end time and the dataset to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'slv'             # slv, flx, adg, or rad\n",
    "start_time = '2000-01-01'   # defaults to 00:00 hrs\n",
    "end_time = '2025-10-01'     # data will be downloaded up to and not including this date\n",
    "\n",
    "# default configuration is to save the urls to a folder named with the dataset\n",
    "data_fp += dataset + '/'\n",
    "\n",
    "# if that path does not exist, create the folder\n",
    "if not os.path.exists(data_fp):\n",
    "    os.mkdir(data_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9450a7f",
   "metadata": {},
   "source": [
    "### 4. Check which files need to be downloaded.\n",
    "\n",
    "The following cell will check if any of the files already exist and compile a list of the dates still needed.\n",
    "\n",
    "When you download the files, you might get an issue where some of them randomly fail. I made this code to check which files are missing, so you will need to iterate through this and the next step until you get the \"Got all files!\" print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days = []\n",
    "for date in pd.date_range(start_time, end_time):\n",
    "    # version numbers change with the year\n",
    "    year = date.year\n",
    "    v = version(year)\n",
    "\n",
    "    # loop through days and check if the file is there\n",
    "    date_fmtd = date.strftime('%Y%m%d')\n",
    "    date_fn = filename.replace('DATE',str(date_fmtd)).replace('DATASET', dataset)\n",
    "\n",
    "    # file can exist under two different version types (e.g., 400 and 401)\n",
    "    if not os.path.exists(data_fp+date_fn.replace('VERSION',v)):\n",
    "        if not os.path.exists(data_fp+date_fn.replace('VERSION',version(year,True))):\n",
    "            missing_days.append(date_fmtd)\n",
    "\n",
    "# print out which days are missing for a sanity check\n",
    "if len(missing_days) > 0:\n",
    "    print(f'Need to download {len(missing_days)} missing files:',missing_days)\n",
    "else:\n",
    "    print('Got all files!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b69340",
   "metadata": {},
   "source": [
    "### 5. Create the new urls file with missing dates\n",
    "\n",
    "The next block will write a new url.txt file which you can then execute from the command-line using `download_urls.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62566ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath to the .txt file you will use to download \n",
    "fn_urls = data_fp + 'urls.txt'\n",
    "\n",
    "# Specify if the new version should be used (see markdown explanation above)\n",
    "NEW_VERSION = False\n",
    "\n",
    "# Integers you determined above from lat/lon bounding box\n",
    "X1 = lon_min_idx\n",
    "X2 = lon_max_idx\n",
    "Y1 = lat_min_idx\n",
    "Y2 = lat_max_idx\n",
    "\n",
    "# URLs for each dataset, already filled out to contain the correct variables from each dataset\n",
    "newlines = {\n",
    "            'slv':'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/YEAR/MONTH/MERRA2_VERSION.tavg1_2d_slv_Nx.DATE.nc4.nc4?PS[0:23][Y1:Y2][X1:X2],QV2M[0:23][Y1:Y2][X1:X2],T2M[0:23][Y1:Y2][X1:X2],U2M[0:23][Y1:Y2][X1:X2],V2M[0:23][Y1:Y2][X1:X2],QV2M[0:23][Y1:Y2][X1:X2],time,lat[Y1:Y2],lon[X1:X2]', # \n",
    "            'rad':'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXRAD.5.12.4/YEAR/MONTH/MERRA2_VERSION.tavg1_2d_rad_Nx.DATE.nc4.nc4?CLDTOT[0:23][Y1:Y2][X1:X2],LWGAB[0:23][Y1:Y2][X1:X2],SWGDN[0:23][Y1:Y2][X1:X2],time,lat[Y1:Y2],lon[X1:X2]', # \n",
    "            'flx':'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXFLX.5.12.4/YEAR/MONTH/MERRA2_VERSION.tavg1_2d_flx_Nx.DATE.nc4.nc4?PRECTOTCORR[0:23][Y1:Y2][X1:X2],time,lat[Y1:Y2],lon[X1:X2]',\n",
    "            'adg':'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXADG.5.12.4/YEAR/MONTH/MERRA2_VERSION.tavg1_2d_adg_Nx.DATE.nc4.nc4?OCDP002[0:23][Y1:Y2][X1:X2],OCWT002[0:23][Y1:Y2][X1:X2],BCDP002[0:23][Y1:Y2][X1:X2],BCWT002[0:23][Y1:Y2][X1:X2],DUDP003[0:23][Y1:Y2][X1:X2],DUWT003[0:23][Y1:Y2][X1:X2]', # \n",
    "}\n",
    "\n",
    "# Loop through missing days and add a new line to the url.txt file for each\n",
    "if len(missing_days) > 0:\n",
    "    f = open(fn_urls, 'w')\n",
    "    for date in missing_days: # [len(missing_days) // 2:]\n",
    "        newline = newlines[dataset]\n",
    "        newline = newline.replace('YEAR',date[:4])\n",
    "        newline = newline.replace('MONTH',date[4:6])\n",
    "        newline = newline.replace('DATE',date)\n",
    "        newline = newline.replace('VERSION',version(int(date[:4]),NEW_VERSION))\n",
    "        newline = newline.replace('X1', str(X1))\n",
    "        newline = newline.replace('X2', str(X2))\n",
    "        newline = newline.replace('Y1', str(Y1))\n",
    "        newline = newline.replace('Y2', str(Y2))\n",
    "        f.write(newline+'\\n')\n",
    "    f.close()\n",
    "    n_missing = len(missing_days)\n",
    "    print(f'Wrote {fn_urls} with {n_missing} urls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d46bf28",
   "metadata": {},
   "source": [
    "### 6. Download files.\n",
    "\n",
    "The following code will loop through each URL and download the data if you just have a few things to grab.\n",
    "\n",
    "Note: the same code is available in script format (`python download_urls.py`) to do large downloads from the command line instead of in a notebook. Flagging `-url_name` with a string or list of strings containing the filepaths of URL text files as generated in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datasets = ['adg','rad','slv','flx'] # List containing any combination of ['adg','rad','slv','flx'] to download in series\n",
    "\n",
    "# Define function to get good filenames\n",
    "def safe_filename(url):\n",
    "    base = urlparse(url).path\n",
    "    return os.path.basename(unquote(base))\n",
    "\n",
    "for dataset in list_datasets:\n",
    "    # Open url file\n",
    "    with open(fn_urls, 'r') as f:\n",
    "        urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    # Download with progress bar\n",
    "    for url in tqdm(urls, desc=f'Downloading files from {dataset}', unit=\"file\"):\n",
    "        filename = os.path.join(data_fp, safe_filename(url))\n",
    "        response = requests.get(url)\n",
    "        with open(filename, 'wb') as out_file:\n",
    "            out_file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251095a7",
   "metadata": {},
   "source": [
    "### Congratulations, you have all the data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
